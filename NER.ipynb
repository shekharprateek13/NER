{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gold_standard(): \n",
    "    \"\"\"\n",
    "        :return : A list of the gold standard named entity status of all dataset tokens [True, False, False,...] \n",
    "    \"\"\"\n",
    "    result=[] \n",
    "    for line in open ( 'ner_dataset.txt' ).readlines(): \n",
    "        if line.strip():\n",
    "            token, part_of_speech, chunk, ne_category=line.split() \n",
    "            result.append(ne_category!= 'O' )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ner(): \n",
    "    \"\"\"\n",
    "        :return : A list of the predicted named entity status of all dataset tokens [True, False, False,...] \n",
    "    \"\"\"\n",
    "    result=[];\n",
    "    sentence_start = True \n",
    "    for line in open ( 'ner_dataset.txt' ).readlines(): \n",
    "        if line.strip():\n",
    "            token,part_of_speech,chunk,ne_category=line.split();\n",
    "            if sentence_start == True:\n",
    "                result.append(False);\n",
    "                sentence_start = False;\n",
    "                continue;\n",
    "            else:\n",
    "                if token[0].isupper():\n",
    "                    result.append(True);\n",
    "                else:\n",
    "                    result.append(False);\n",
    "        else : \n",
    "            sentence_start = True \n",
    "    return result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Source: http://www.nltk.org/book/ch07.html\n",
    "#Problem 1: Extra Credits\n",
    "def improved_ner(): \n",
    "    \"\"\"\n",
    "        :return : A list of the predicted named entity status of all dataset tokens [True, False, False,...] \n",
    "    \"\"\"\n",
    "    result=[];\n",
    "    for line in open ( 'ner_dataset.txt' ).readlines(): \n",
    "        isNamedEntity = False;\n",
    "        if line.strip():\n",
    "            try:\n",
    "                token,part_of_speech,chunk,ne_category=line.split();\n",
    "                tokenized_token = nltk.word_tokenize(token);\n",
    "                pos_tagged = nltk.pos_tag(tokenized_token);\n",
    "                chunk_tree = nltk.ne_chunk(pos_tagged,binary=True);\n",
    "                for node in chunk_tree.subtrees():\n",
    "                    if node.label() == 'NE':\n",
    "                        isNamedEntity = True;\n",
    "                        result.append(True);\n",
    "                        \n",
    "                if isNamedEntity == False:\n",
    "                    result.append(False);\n",
    "                    isNamedEntity = False;\n",
    "            except Exception:\n",
    "                print line;\n",
    "    return result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem 2: Evaluation function\n",
    "def evaluate(gold_standard_result, prediction_result):\n",
    "    if len(gold_standard_result) == len(prediction_result):\n",
    "        true_positive = 0.0;\n",
    "        false_positive = 0.0;\n",
    "        true_negative = 0.0;\n",
    "        false_negative = 0.0;\n",
    "        for i in range(0,len(prediction_result)):\n",
    "            predicted_ner = prediction_result[i];\n",
    "            gold_standard_ner = gold_standard_result[i];\n",
    "            if gold_standard_ner == True and predicted_ner == True:\n",
    "                true_positive += 1;\n",
    "            elif gold_standard_ner == True and predicted_ner == False:\n",
    "                false_negative += 1;\n",
    "            elif gold_standard_ner == False and predicted_ner == False:\n",
    "                true_negative += 1;\n",
    "            else:\n",
    "                false_positive += 1;\n",
    "\n",
    "        precision = true_positive/(true_positive+false_positive);\n",
    "        recall = true_positive/(true_positive+false_negative);\n",
    "        f1_score = (2*precision*recall)/(precision+recall);\n",
    "        print \"Precision: \" + repr(precision);\n",
    "        print \"Recall:\" + repr(recall);\n",
    "        print \"F1-Score:\" + repr(f1_score);\n",
    "    else:\n",
    "        print \"Error\";\n",
    "    return;    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem 2: Strict measure method - Extra Credits\n",
    "def evaluate_strict(gold_standard_result, prediction_result):\n",
    "    if len(gold_standard_result) == len(prediction_result):\n",
    "        true_positive = 0.0;\n",
    "        false_positive = 0.0;\n",
    "        true_negative = 0.0;\n",
    "        false_negative = 0.0;\n",
    "        length = len(gold_standard_result);\n",
    "       \n",
    "        gold_standard_named_entity_list = [];\n",
    "        gold_standard_negative_list = [];\n",
    "        \n",
    "        i = 0;\n",
    "        j = 0;\n",
    "        while(i < length):\n",
    "            if(gold_standard_result[i] == True and prediction_result[i] == True):\n",
    "                startIndex = i;\n",
    "                endIndex = i;\n",
    "                while(endIndex < length and gold_standard_result[endIndex] == True):\n",
    "                    endIndex = endIndex + 1;\n",
    "                \n",
    "                while(startIndex < endIndex):\n",
    "                    if(prediction_result[startIndex] == False):\n",
    "                        break;\n",
    "                    else:\n",
    "                        startIndex = startIndex + 1;\n",
    "               \n",
    "                if(startIndex == endIndex):\n",
    "                    true_positive = true_positive + 1;\n",
    "                i = endIndex;\n",
    "            elif(gold_standard_result[i] == True and prediction_result[i] == False):\n",
    "                startIndex = i;\n",
    "                endIndex = i;\n",
    "                while(endIndex < length and gold_standard_result[endIndex] == True):\n",
    "                    endIndex = endIndex + 1;    \n",
    "                \n",
    "                if(startIndex == (endIndex-1)):\n",
    "                    false_negative = false_negative + 1;\n",
    "                \n",
    "                i = endIndex;                \n",
    "            elif(gold_standard_result[i] == False and prediction_result[i] == True):\n",
    "                i += 1;\n",
    "                false_positive = false_positive + 1;\n",
    "            else:\n",
    "                i += 1;\n",
    "        precision = true_positive/(true_positive+false_positive);\n",
    "        recall = true_positive/(true_positive+false_negative);\n",
    "        f1_score = (2*precision*recall)/(precision+recall);\n",
    "        print \"TP: \" + repr(true_positive);\n",
    "        print \"FP:\" + repr(false_positive);\n",
    "        print \"TN:\" + repr(true_negative);\n",
    "        print \"FN:\" + repr(false_negative);\n",
    "        print \"Precision: \" + repr(precision);\n",
    "        print \"Recall:\" + repr(recall);\n",
    "        print \"F1-Score:\" + repr(f1_score);\n",
    "    else:\n",
    "        print \"Error\";\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem 2: Lenient measure method - Extra Credits\n",
    "def evaluate_lenient(gold_standard_result, prediction_result):\n",
    "    if len(gold_standard_result) == len(prediction_result):\n",
    "        true_positive = 0.0;\n",
    "        false_positive = 0.0;\n",
    "        true_negative = 0.0;\n",
    "        false_negative = 0.0;\n",
    "        length = len(gold_standard_result);\n",
    "       \n",
    "        gold_standard_named_entity_list = [];\n",
    "        gold_standard_negative_list = [];\n",
    "        \n",
    "        i = 0;\n",
    "        j = 0;\n",
    "        while(i < length):\n",
    "            if(gold_standard_result[i] == True and prediction_result[i] == True):\n",
    "                startIndex = i;\n",
    "                endIndex = i;\n",
    "                while(endIndex < length and gold_standard_result[endIndex] == True):\n",
    "                    endIndex = endIndex + 1;\n",
    "                \n",
    "                while(startIndex < endIndex):\n",
    "                    if(prediction_result[startIndex] == False):\n",
    "                        break;\n",
    "                    else:\n",
    "                        startIndex = startIndex + 1;\n",
    "               \n",
    "                if(startIndex <= endIndex):\n",
    "                    true_positive = true_positive + 1;\n",
    "                i = endIndex;\n",
    "            elif(gold_standard_result[i] == True and prediction_result[i] == False):\n",
    "                i = i + 1;\n",
    "                false_negative = false_negative + 1;       \n",
    "            elif(gold_standard_result[i] == False and prediction_result[i] == True):\n",
    "                i += 1;\n",
    "                false_positive = false_positive + 1;\n",
    "            else:\n",
    "                i += 1;\n",
    "        precision = true_positive/(true_positive+false_positive);\n",
    "        recall = true_positive/(true_positive+false_negative);\n",
    "        f1_score = (2*precision*recall)/(precision+recall);\n",
    "        print \"TP: \" + repr(true_positive);\n",
    "        print \"FP:\" + repr(false_positive);\n",
    "        print \"TN:\" + repr(true_negative);\n",
    "        print \"FN:\" + repr(false_negative);\n",
    "        print \"Precision: \" + repr(precision);\n",
    "        print \"Recall:\" + repr(recall);\n",
    "        print \"F1-Score:\" + repr(f1_score);\n",
    "    else:\n",
    "        print \"Error\";\n",
    "    return; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gold_standard_result=gold_standard() \n",
    "prediction_result=ner()\n",
    "improved_prediction_result = improved_ner();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46666\n",
      "46666\n",
      "46666\n"
     ]
    }
   ],
   "source": [
    "print len(gold_standard_result);\n",
    "print len(prediction_result);\n",
    "print len(improved_prediction_result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall & F1-Score from original NER:\n",
      "Precision: 0.7890855457227138\n",
      "Recall:0.7914201183431953\n",
      "F1-Score:0.7902511078286558\n",
      "\n",
      "Precision, Recall & F1-Score from improved NER:\n",
      "Precision: 0.8682933228387774\n",
      "Recall:0.8159516765285996\n",
      "F1-Score:0.8413091833492214\n"
     ]
    }
   ],
   "source": [
    "print \"Precision, Recall & F1-Score from original NER:\"\n",
    "evaluate(gold_standard_result, prediction_result)\n",
    "print \"\\nPrecision, Recall & F1-Score from improved NER:\"\n",
    "evaluate(gold_standard_result, improved_prediction_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 4230.0\n",
      "FP:1004.0\n",
      "TN:0.0\n",
      "FN:509.0\n",
      "Precision: 0.8081773022544899\n",
      "Recall:0.8925933741295632\n",
      "F1-Score:0.8482903840368997\n"
     ]
    }
   ],
   "source": [
    "#Problem 2: Strict measure\n",
    "evaluate_strict(gold_standard_result, improved_prediction_result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 4865.0\n",
      "FP:1004.0\n",
      "TN:0.0\n",
      "FN:991.0\n",
      "Precision: 0.8289316749020276\n",
      "Recall:0.8307718579234973\n",
      "F1-Score:0.8298507462686566\n"
     ]
    }
   ],
   "source": [
    "#Problem 2: Lenient measure\n",
    "evaluate_lenient(gold_standard_result, improved_prediction_result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
